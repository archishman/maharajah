{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import chess\n",
    "import chess.pgn\n",
    "import chess.engine\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(pgn, max_games = 1000):\n",
    "    games = []\n",
    "    with open(pgn,'r+') as g:\n",
    "        for i in range(max_games):\n",
    "            gm = chess.pgn.read_game(g)\n",
    "            if (gm == None):\n",
    "                break\n",
    "            else:\n",
    "                games.append(gm)\n",
    "    return games\n",
    "def random_game_node(game):\n",
    "    boards = list(map(chess.pgn.GameNode.board, game.mainline()))\n",
    "    return random.choice(boards)\n",
    "def pick_positions(games):\n",
    "    return list(map(random_game_node, games))\n",
    "def tensorize(b):\n",
    "    t = np.zeros((18, 8,8))\n",
    "    temp = np.asarray(list(map(list, str(b).replace(' ','').split('\\n'))))\n",
    "    pieces = list('KQRBNP.kqrbnp')\n",
    "    for i in range(len(pieces)):\n",
    "        piece = pieces[i]\n",
    "        is_piece = temp == [piece] * 8\n",
    "        t[i] = np.asarray(is_piece)\n",
    "    t[13] = np.ones(t[13].shape) if (b.turn == chess.WHITE) else t[13]\n",
    "    t[14] = np.ones(t[14].shape) if (b.has_queenside_castling_rights(chess.WHITE)) else t[14]\n",
    "    t[15] = np.ones(t[15].shape) if (b.has_queenside_castling_rights(chess.BLACK)) else t[15]\n",
    "    t[16] = np.ones(t[16].shape) if (b.has_kingside_castling_rights(chess.WHITE)) else t[16]    \n",
    "    t[17] = np.ones(t[17].shape) if (b.has_kingside_castling_rights(chess.BLACK)) else t[17]\n",
    "    return t\n",
    "def tensorize_boards(boards):\n",
    "    return list(map(tensorize, boards))\n",
    "def eval_with_engine(engine):    \n",
    "    return (lambda board: (engine.analyse(board, chess.engine.Limit(time=0.100)))['score'].white().score(mate_score=10000))\n",
    "def get_evals(boards, engine):\n",
    "    return list(map(eval_with_engine(engine), boards))\n",
    "def get_data(pgn='2013-01-games.pgn', engine_path = './stockfish.exe'):\n",
    "    games = read(pgn)\n",
    "    boards = pick_positions(games)\n",
    "    engine =  chess.engine.SimpleEngine.popen_uci(engine_path)\n",
    "    x = np.asarray(tensorize_boards(boards))\n",
    "    y = np.asarray(get_evals(boards, engine))\n",
    "    engine.quit()\n",
    "    return x,y/100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 1.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 1.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 1., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 1.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 1., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 1., 0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 1., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 1.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 1.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "         [[1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.],\n",
       "          [1., 1., 1., ..., 1., 1., 1.]],\n",
       " \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]]]]),\n",
       " array([ 3.800e-01,  5.700e-01,  4.500e+00, -1.820e+00, -1.378e+01,\n",
       "         8.890e+00,  7.000e-01,  1.370e+00,  1.438e+01, -1.063e+01,\n",
       "        -1.070e+00,  9.000e-02,  2.340e+00,  8.700e-01,  1.790e+00,\n",
       "         2.770e+00, -2.690e+00,  1.960e+00,  1.900e-01,  2.140e+00,\n",
       "         2.830e+00,  9.900e-01,  1.400e+00,  2.200e+00,  1.520e+00,\n",
       "         3.670e+00, -1.560e+00, -4.600e-01,  1.135e+01,  2.870e+00,\n",
       "        -1.630e+00,  3.900e-01, -5.000e-01,  3.200e-01, -9.000e-02,\n",
       "         8.500e-01, -3.000e-02,  9.998e+01,  3.620e+00,  2.120e+00,\n",
       "         6.600e-01,  5.400e-01, -8.600e-01, -4.500e-01, -1.530e+00,\n",
       "        -5.970e+00,  3.340e+00,  1.686e+01, -2.500e-01,  1.000e+02,\n",
       "        -1.060e+00,  2.900e-01, -1.420e+00, -1.380e+00, -7.420e+00,\n",
       "        -8.000e-01,  3.310e+00,  7.600e-01,  8.200e-01, -4.640e+00,\n",
       "        -1.880e+00,  3.700e-01,  2.086e+01,  4.830e+00,  2.700e-01,\n",
       "        -5.400e+00,  8.500e-01,  2.930e+00, -7.400e-01,  3.500e-01,\n",
       "        -1.712e+01,  1.430e+00,  1.320e+00, -6.400e-01, -3.710e+00,\n",
       "         2.650e+00, -1.293e+01,  3.200e-01,  5.670e+00,  3.110e+00,\n",
       "         7.300e-01, -3.470e+00,  5.100e-01,  3.570e+00,  1.690e+00,\n",
       "         1.320e+00,  9.999e+01,  1.269e+01,  8.080e+00,  2.540e+00,\n",
       "         1.490e+01,  9.999e+01, -2.180e+00,  3.800e-01, -2.040e+00,\n",
       "         5.000e-01, -9.600e-01, -3.000e-01, -2.700e-01,  5.740e+00,\n",
       "         9.100e-01,  7.660e+00,  1.150e+00,  1.080e+00,  1.795e+01,\n",
       "        -2.514e+01,  2.440e+00, -6.400e+00, -9.900e-01, -5.400e-01,\n",
       "        -2.380e+00,  1.694e+01,  1.200e+00, -5.900e+00, -2.200e-01,\n",
       "         4.660e+00,  9.996e+01, -2.666e+01,  2.100e-01,  6.050e+00,\n",
       "        -5.030e+00, -2.810e+00, -2.009e+01, -1.600e-01, -7.700e-01,\n",
       "        -4.460e+00, -6.200e-01,  1.240e+00,  1.907e+01, -1.500e-01,\n",
       "         7.100e-01,  2.300e-01,  9.400e-01, -1.180e+00,  9.900e+00,\n",
       "         2.600e-01, -9.996e+01,  6.780e+00,  1.830e+00, -1.789e+01,\n",
       "         1.580e+00, -1.430e+00, -9.500e-01,  9.995e+01, -1.500e-01,\n",
       "         8.000e-02,  4.620e+00, -9.993e+01, -9.960e+00,  9.994e+01,\n",
       "        -5.050e+00,  5.340e+00,  1.240e+00,  5.700e-01, -3.990e+00,\n",
       "         4.610e+00,  7.900e-01,  6.500e-01, -3.300e-01,  3.700e-01,\n",
       "        -5.800e-01, -1.993e+01, -9.520e+00,  9.997e+01,  8.640e+00,\n",
       "         0.000e+00,  1.248e+01, -7.900e-01,  9.400e-01, -9.000e-02,\n",
       "         2.360e+00, -2.983e+01,  9.997e+01,  2.670e+00,  8.480e+00,\n",
       "         4.160e+00,  1.948e+01, -6.520e+00,  9.910e+00,  4.880e+00,\n",
       "         7.750e+00, -7.100e-01,  1.660e+00,  4.030e+00, -1.500e+00,\n",
       "        -8.000e-01,  7.400e+00,  3.810e+00,  9.100e-01, -9.540e+00,\n",
       "         1.190e+00, -1.300e-01,  9.400e-01, -2.100e-01, -3.070e+00,\n",
       "         1.640e+00,  4.000e-02,  6.900e-01,  1.690e+00,  9.999e+01,\n",
       "        -1.000e-02,  1.499e+01,  6.500e+00,  6.950e+00,  1.900e-01,\n",
       "        -4.860e+00,  2.930e+00,  1.770e+00,  7.540e+00, -1.410e+00,\n",
       "         1.870e+00,  2.300e-01, -9.000e-02, -8.460e+00, -8.000e-02,\n",
       "         1.560e+00,  6.300e-01,  8.100e-01,  3.000e-01, -5.100e-01,\n",
       "         5.250e+00,  2.990e+00, -4.450e+00, -1.020e+00,  6.000e-01,\n",
       "        -2.840e+00,  1.416e+01, -2.300e-01,  1.610e+00, -9.998e+01,\n",
       "         8.500e-01,  1.620e+00,  6.190e+00, -1.042e+01,  1.656e+01,\n",
       "         3.700e-01, -2.310e+00,  8.600e-01,  2.700e-01,  1.490e+00,\n",
       "         9.991e+01, -1.208e+01,  2.000e-01,  2.070e+00,  1.877e+01,\n",
       "         1.600e-01,  1.091e+01, -1.552e+01,  6.910e+00,  7.200e-01,\n",
       "         1.510e+01,  1.170e+00, -8.250e+00, -9.070e+00,  4.400e+00,\n",
       "         3.000e-01, -8.500e-01,  9.110e+00, -1.700e-01,  6.186e+01,\n",
       "         8.900e-01,  1.200e+00,  2.530e+00, -2.400e-01,  1.280e+00,\n",
       "        -4.930e+00,  4.600e-01, -1.700e-01, -8.300e-01,  1.050e+00,\n",
       "         1.100e-01, -2.790e+00,  1.196e+01,  6.900e-01, -8.330e+00,\n",
       "        -7.720e+00,  9.000e-02,  0.000e+00, -4.200e-01, -2.000e-01,\n",
       "        -1.800e-01, -1.000e+00, -1.100e-01,  1.540e+00,  1.440e+00,\n",
       "         1.900e-01,  0.000e+00,  8.700e-01, -1.126e+01, -4.000e+00,\n",
       "         9.800e-01, -7.300e-01, -3.880e+00, -3.080e+00,  1.390e+00,\n",
       "        -5.300e-01,  3.800e-01, -7.500e-01,  7.350e+00,  1.260e+00,\n",
       "        -1.200e+00, -3.610e+00,  8.390e+00, -1.800e-01, -2.680e+00,\n",
       "         3.490e+00, -8.600e-01,  5.570e+00, -5.680e+00,  4.800e-01,\n",
       "        -7.600e-01,  1.090e+00, -4.050e+00, -9.996e+01,  7.700e-01,\n",
       "         1.390e+00,  4.950e+00, -1.540e+00,  2.600e+00,  1.240e+00,\n",
       "         9.999e+01,  1.040e+00, -1.303e+01,  3.700e-01,  4.490e+00,\n",
       "        -1.140e+00, -2.800e-01, -4.000e+00,  9.997e+01,  9.996e+01,\n",
       "        -3.810e+00,  2.500e-01,  5.690e+00,  1.190e+00, -4.620e+00,\n",
       "         1.520e+00,  3.800e-01,  6.640e+00, -1.975e+01, -9.996e+01,\n",
       "         2.700e-01,  4.600e-01,  3.530e+00,  7.560e+00,  7.140e+00,\n",
       "        -3.970e+00,  2.940e+00, -4.550e+00,  5.980e+00,  6.200e-01,\n",
       "        -2.000e+00,  7.800e-01,  7.500e-01, -7.568e+01,  1.710e+00,\n",
       "         5.740e+00, -1.600e-01,  4.000e-01,  1.290e+00, -2.030e+00,\n",
       "         1.970e+00, -1.630e+00, -1.063e+01,  1.780e+00,  1.070e+00,\n",
       "         2.300e-01, -9.992e+01, -8.600e-01, -1.400e-01,  6.686e+01,\n",
       "         1.150e+00, -6.320e+00, -1.360e+00, -3.050e+00, -4.600e-01,\n",
       "        -3.335e+01,  2.030e+00, -2.030e+01, -6.200e-01, -1.000e+00,\n",
       "         6.100e-01,  6.000e-02,  1.840e+01, -5.000e-01, -3.730e+00,\n",
       "        -8.320e+00,  1.120e+00, -3.230e+00, -5.330e+00,  7.200e-01,\n",
       "         9.400e-01, -1.799e+01,  3.000e-01,  2.000e-02,  7.700e-01,\n",
       "        -7.980e+00,  4.470e+00, -2.940e+00,  8.000e-02,  1.300e-01,\n",
       "         1.110e+00, -1.790e+00,  2.510e+00, -2.700e-01,  4.800e-01,\n",
       "         1.025e+01,  1.030e+00,  1.500e-01,  9.988e+01, -1.282e+01,\n",
       "         1.330e+00, -8.900e-01,  7.460e+00,  1.950e+00,  1.333e+01,\n",
       "         6.270e+00, -1.467e+01,  2.370e+00, -3.150e+00,  4.880e+00,\n",
       "         1.300e+00,  2.690e+00,  4.800e-01,  4.110e+00,  9.850e+00,\n",
       "        -3.200e-01,  7.880e+00, -1.370e+00, -4.000e+00,  1.320e+00,\n",
       "        -1.160e+00,  4.580e+00, -1.400e+00, -2.290e+00,  9.520e+00,\n",
       "         5.900e-01, -1.440e+00, -2.130e+00, -3.250e+00, -9.000e-02,\n",
       "        -9.400e-01,  0.000e+00,  9.780e+00, -2.370e+00, -4.800e-01,\n",
       "         4.640e+00, -5.900e+00, -1.563e+01,  4.330e+00,  7.070e+00,\n",
       "         5.710e+00, -9.996e+01, -5.000e-02, -1.120e+00,  1.030e+00,\n",
       "         6.210e+01, -5.090e+00, -3.600e-01, -2.000e-02,  1.250e+00,\n",
       "         1.620e+00, -6.700e-01,  5.600e-01, -1.000e-02, -3.800e-01,\n",
       "         9.998e+01,  1.462e+01, -1.200e-01,  2.570e+00, -6.310e+00,\n",
       "         8.360e+00, -2.000e+00,  1.700e-01, -9.999e+01,  2.260e+00,\n",
       "         9.999e+01,  1.690e+00, -1.070e+00,  4.300e-01, -1.050e+01,\n",
       "         2.050e+00, -9.270e+00, -7.010e+00, -3.000e-01, -1.060e+00,\n",
       "        -3.300e-01, -5.100e-01,  1.460e+00,  1.180e+00,  7.400e-01,\n",
       "        -8.600e-01,  2.230e+00, -2.980e+00,  4.870e+00, -6.590e+00,\n",
       "         1.600e-01,  2.300e-01, -3.140e+00,  1.300e+00, -5.140e+00,\n",
       "        -1.100e+00, -1.200e-01, -6.679e+01,  9.300e-01,  9.997e+01,\n",
       "        -9.999e+01,  8.170e+00, -9.000e-02,  4.450e+00, -3.600e-01,\n",
       "         2.300e-01,  1.260e+00,  1.099e+01,  3.030e+00, -3.270e+00,\n",
       "         1.600e+00, -1.190e+00, -2.000e-02, -2.660e+00,  5.000e+00,\n",
       "         1.490e+00, -4.500e-01,  1.700e-01,  1.463e+01,  9.300e-01,\n",
       "        -1.130e+01,  1.810e+00, -1.526e+01,  2.900e-01, -2.800e-01,\n",
       "        -4.040e+00, -3.100e-01,  1.760e+00,  8.600e-01,  9.998e+01,\n",
       "         8.700e-01,  1.050e+00, -3.900e-01,  3.970e+00, -4.900e-01,\n",
       "        -1.079e+01,  8.100e+00,  2.065e+01,  1.860e+00,  9.996e+01,\n",
       "        -4.300e-01,  1.300e-01,  8.000e-01,  4.800e-01, -6.000e-01,\n",
       "         2.000e-01,  1.670e+00, -4.100e+00, -1.130e+00,  1.370e+00,\n",
       "         9.995e+01, -9.400e-01,  1.200e-01, -6.330e+00,  8.820e+00,\n",
       "        -1.400e-01, -1.010e+01,  5.300e-01,  5.300e-01, -2.890e+00,\n",
       "        -7.980e+00,  4.200e-01, -3.200e-01, -8.490e+00,  1.080e+00,\n",
       "         1.669e+01, -7.000e-02,  2.560e+00,  8.300e-01,  5.300e-01,\n",
       "        -1.880e+00,  7.190e+00, -3.340e+00, -1.200e+00,  3.810e+00,\n",
       "        -1.048e+01,  7.300e-01,  3.109e+01,  6.240e+00,  1.457e+01,\n",
       "        -3.900e-01,  0.000e+00, -3.600e-01,  2.410e+00, -1.000e-02,\n",
       "         3.500e-01,  9.999e+01,  1.044e+01, -7.700e-01, -1.670e+00,\n",
       "        -4.000e-01, -1.340e+00,  1.200e-01,  3.260e+00,  3.590e+00,\n",
       "         1.040e+00, -2.570e+00, -2.000e-01,  5.000e-01,  6.200e-01,\n",
       "         2.300e-01, -1.000e-01, -1.700e-01,  6.700e-01, -1.040e+00,\n",
       "        -1.677e+01,  5.130e+00,  8.070e+00,  4.630e+00, -2.240e+00,\n",
       "        -1.043e+01,  9.870e+00,  3.000e+00, -2.560e+00, -5.200e-01,\n",
       "         1.920e+00, -1.720e+00,  1.320e+00, -8.400e-01,  1.210e+00,\n",
       "         6.640e+00,  7.400e-01,  1.380e+00, -8.400e-01,  5.360e+00,\n",
       "         7.400e-01, -1.276e+01, -1.080e+00,  7.000e-01,  8.900e-01,\n",
       "         6.100e-01,  1.490e+00,  1.500e-01, -3.370e+00,  3.350e+00,\n",
       "         2.508e+01,  3.440e+00,  7.600e-01, -1.180e+00,  8.960e+00,\n",
       "        -9.997e+01,  1.200e-01,  2.600e-01,  4.380e+00,  1.700e+00,\n",
       "        -2.000e-01, -4.680e+00, -4.200e-01, -1.850e+00, -5.100e+00,\n",
       "        -2.900e-01,  5.800e-01,  9.000e-02,  2.340e+00,  2.490e+00,\n",
       "        -7.500e-01,  1.670e+01,  0.000e+00,  8.000e-02,  2.470e+00,\n",
       "        -7.900e-01,  9.240e+00, -2.530e+00,  1.280e+00, -6.920e+00,\n",
       "         8.510e+00, -3.400e-01,  2.403e+01, -1.940e+00, -2.660e+00,\n",
       "         3.080e+00,  8.300e-01,  1.460e+00, -9.991e+01,  2.280e+00,\n",
       "        -1.002e+01, -2.120e+00,  2.615e+01, -1.889e+01,  4.080e+00,\n",
       "         2.800e+00,  6.193e+01, -1.230e+00, -9.000e-02, -1.030e+00,\n",
       "         9.420e+00, -2.010e+00,  7.970e+00, -1.070e+00,  1.000e-02,\n",
       "        -9.100e-01,  7.720e+00, -2.000e+00,  1.260e+00,  0.000e+00,\n",
       "         9.500e-01,  4.890e+00,  4.000e-01,  4.500e-01,  1.801e+01,\n",
       "        -1.000e-01,  1.000e-02,  1.300e-01,  5.850e+00, -2.570e+00,\n",
       "         1.700e-01,  3.100e-01,  7.580e+00,  1.000e-01,  2.600e-01,\n",
       "        -1.113e+01,  9.000e-02, -1.758e+01,  1.432e+01, -7.800e-01,\n",
       "        -6.114e+01, -1.704e+01,  4.000e-02, -5.820e+00, -6.400e-01,\n",
       "        -2.700e-01, -6.180e+00,  2.080e+00,  1.220e+00, -8.620e+00,\n",
       "         1.440e+00,  1.530e+00, -4.200e-01,  6.000e-01,  1.466e+01,\n",
       "        -1.260e+00, -2.500e-01, -9.997e+01, -7.400e-01, -7.400e-01,\n",
       "        -2.500e-01,  1.600e+00, -1.780e+00, -5.330e+00, -3.000e-02,\n",
       "        -5.100e-01,  2.000e-02, -1.140e+00, -6.000e-02,  9.100e-01,\n",
       "         9.999e+01,  2.630e+00,  9.350e+00, -3.500e-01, -1.610e+00,\n",
       "        -1.850e+00, -2.793e+01,  2.200e-01,  3.370e+00, -7.500e-01,\n",
       "        -4.280e+00,  1.700e+00,  1.750e+00,  9.998e+01, -1.378e+01,\n",
       "         2.050e+00,  6.300e-01,  7.540e+00, -1.680e+00,  1.940e+00,\n",
       "         9.994e+01, -2.010e+00,  6.000e-01, -7.500e-01,  2.070e+00,\n",
       "        -2.710e+00,  1.250e+00, -3.540e+00,  5.889e+01, -1.170e+00,\n",
       "        -8.710e+00,  1.660e+00,  2.800e-01, -5.490e+00, -5.060e+00,\n",
       "        -2.730e+00, -1.480e+00, -4.690e+00,  5.840e+00,  9.999e+01,\n",
       "        -7.400e+00,  4.700e-01,  3.540e+00, -1.940e+00,  6.100e-01,\n",
       "         6.600e-01,  3.270e+00, -7.500e+00, -6.530e+00,  2.160e+00,\n",
       "         1.270e+00,  1.300e-01,  1.130e+00,  4.200e-01, -2.100e+00,\n",
       "         1.132e+01,  2.271e+01,  7.600e-01,  1.910e+00,  6.700e-01,\n",
       "        -8.700e-01, -7.400e+00,  5.000e-01, -2.420e+00,  1.200e+00,\n",
       "         7.200e-01, -2.980e+00,  1.370e+00, -2.000e-01,  2.630e+00,\n",
       "         8.600e-01,  8.300e-01,  6.200e-01, -5.500e-01, -1.980e+00,\n",
       "         1.179e+01,  1.190e+00, -1.800e+00,  3.290e+00,  2.290e+00,\n",
       "         4.100e-01,  2.870e+00,  2.980e+00, -7.720e+00, -2.600e-01,\n",
       "         5.920e+00,  4.400e-01,  2.000e+00,  7.480e+00,  3.200e-01,\n",
       "        -1.110e+00,  1.120e+01,  1.460e+00,  4.100e-01, -9.995e+01,\n",
       "         7.910e+00,  6.490e+00, -9.998e+01, -9.000e-01, -8.000e-01,\n",
       "         2.260e+00, -2.150e+00,  2.600e-01,  2.000e-02,  4.400e-01,\n",
       "         1.670e+00,  1.930e+00,  7.000e-01, -2.820e+00,  0.000e+00,\n",
       "         9.400e-01,  1.002e+01, -2.620e+00, -8.000e-02,  8.600e-01,\n",
       "         5.430e+00,  5.150e+00, -1.570e+00,  4.400e-01,  4.600e-01,\n",
       "         6.160e+00, -5.110e+00,  1.910e+00,  6.100e-01,  1.000e+00,\n",
       "         2.470e+00, -2.440e+00, -1.020e+00,  0.000e+00, -1.362e+01,\n",
       "        -8.130e+00,  9.995e+01,  1.950e+00, -6.200e-01, -9.997e+01,\n",
       "         1.000e-02,  1.380e+00, -1.920e+00, -5.200e-01, -5.800e-01,\n",
       "        -4.600e-01, -8.700e-01, -9.260e+00, -7.740e+00, -4.390e+00,\n",
       "         4.700e-01,  8.270e+00,  9.600e-01,  3.330e+00, -1.688e+01,\n",
       "         2.390e+00,  3.010e+00,  5.220e+00,  3.070e+00,  4.390e+00,\n",
       "         2.410e+00,  1.830e+00,  1.198e+01, -4.270e+00,  1.700e-01,\n",
       "         2.880e+00, -5.600e-01, -7.600e-01, -3.240e+00, -8.580e+00,\n",
       "         4.750e+00,  7.800e+00, -8.000e-02,  3.810e+00, -1.200e+00,\n",
       "        -2.100e-01, -3.330e+00, -2.350e+00,  9.380e+00, -1.280e+00,\n",
       "         3.800e-01,  3.448e+01, -5.310e+00,  4.400e+00, -3.050e+00,\n",
       "        -7.530e+00, -1.290e+00, -1.470e+00, -2.008e+01, -3.700e-01,\n",
       "        -6.800e-01,  2.840e+00, -3.400e-01,  1.460e+00,  1.380e+00,\n",
       "        -2.128e+01,  9.700e-01, -1.261e+01, -7.890e+00,  1.390e+00,\n",
       "        -3.470e+00,  7.860e+00,  4.130e+00,  8.000e-01,  1.968e+01,\n",
       "         6.510e+00,  9.810e+00,  8.500e-01, -6.200e-01, -2.730e+00,\n",
       "         3.470e+00, -4.300e-01,  1.500e+00, -4.250e+00, -5.800e-01,\n",
       "        -6.900e+00,  7.530e+00, -1.670e+00,  6.360e+00, -5.200e-01,\n",
       "         7.970e+00, -6.600e-01,  3.100e-01,  1.810e+00,  1.240e+00,\n",
       "         1.567e+01, -6.800e-01, -1.700e-01,  6.200e+00,  3.600e+00,\n",
       "        -8.000e-02, -3.300e-01,  5.550e+00,  7.300e-01, -5.000e-01,\n",
       "        -5.220e+00,  1.400e-01, -7.100e-01, -2.080e+00,  6.170e+00,\n",
       "         1.370e+00, -4.850e+00, -7.290e+00, -3.490e+00, -3.400e+00,\n",
       "         7.090e+00,  3.000e-02,  1.600e+00, -6.300e-01, -1.358e+01,\n",
       "         1.070e+00, -4.300e-01,  8.010e+00, -4.000e-01,  9.996e+01]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = get_data()\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x.npy',x)\n",
    "np.save('y.npy',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('x.npy')\n",
    "x = x.reshape((1000,8,8,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load('y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = x[:700], y[:700], x[700:850], y[700:850], x[850:], y[850:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Flatten(input_shape = (8,8,18)))\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 8, 8, 1)           19        \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 84\n",
      "Trainable params: 84\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 150 samples\n",
      "Epoch 1/1000\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 543.7983 - mean_absolute_error: 8.8418 - val_loss: 545.7390 - val_mean_absolute_error: 8.7396\n",
      "Epoch 2/1000\n",
      "700/700 [==============================] - 0s 95us/step - loss: 542.7345 - mean_absolute_error: 8.7983 - val_loss: 545.9272 - val_mean_absolute_error: 8.7552\n",
      "Epoch 3/1000\n",
      "700/700 [==============================] - 0s 95us/step - loss: 541.9853 - mean_absolute_error: 8.7845 - val_loss: 546.0622 - val_mean_absolute_error: 8.7699\n",
      "Epoch 4/1000\n",
      "700/700 [==============================] - 0s 102us/step - loss: 541.1845 - mean_absolute_error: 8.8133 - val_loss: 546.2682 - val_mean_absolute_error: 8.7748\n",
      "Epoch 5/1000\n",
      "700/700 [==============================] - 0s 97us/step - loss: 540.4399 - mean_absolute_error: 8.8209 - val_loss: 546.6632 - val_mean_absolute_error: 8.7651\n",
      "Epoch 6/1000\n",
      "700/700 [==============================] - 0s 86us/step - loss: 539.6060 - mean_absolute_error: 8.7953 - val_loss: 546.6810 - val_mean_absolute_error: 8.8408\n",
      "Epoch 7/1000\n",
      "700/700 [==============================] - 0s 89us/step - loss: 538.7791 - mean_absolute_error: 8.8356 - val_loss: 547.2317 - val_mean_absolute_error: 8.8704\n",
      "Epoch 8/1000\n",
      "700/700 [==============================] - 0s 98us/step - loss: 537.9237 - mean_absolute_error: 8.8814 - val_loss: 547.8216 - val_mean_absolute_error: 8.8548\n",
      "Epoch 9/1000\n",
      "700/700 [==============================] - 0s 99us/step - loss: 537.1868 - mean_absolute_error: 8.8360 - val_loss: 548.0125 - val_mean_absolute_error: 8.9502\n",
      "Epoch 10/1000\n",
      "700/700 [==============================] - 0s 95us/step - loss: 536.5042 - mean_absolute_error: 8.8584 - val_loss: 548.7886 - val_mean_absolute_error: 8.9722\n",
      "Epoch 11/1000\n",
      "700/700 [==============================] - 0s 97us/step - loss: 535.5342 - mean_absolute_error: 8.8967 - val_loss: 549.3720 - val_mean_absolute_error: 8.9567\n",
      "Epoch 12/1000\n",
      "700/700 [==============================] - 0s 101us/step - loss: 535.0494 - mean_absolute_error: 8.8948 - val_loss: 549.6164 - val_mean_absolute_error: 9.0030\n",
      "Epoch 13/1000\n",
      "700/700 [==============================] - 0s 99us/step - loss: 534.2748 - mean_absolute_error: 8.9266 - val_loss: 550.0750 - val_mean_absolute_error: 8.9994\n",
      "Epoch 14/1000\n",
      "700/700 [==============================] - 0s 96us/step - loss: 533.6737 - mean_absolute_error: 8.9275 - val_loss: 550.3836 - val_mean_absolute_error: 9.0321\n",
      "Epoch 15/1000\n",
      "700/700 [==============================] - 0s 103us/step - loss: 532.8462 - mean_absolute_error: 8.9591 - val_loss: 550.7408 - val_mean_absolute_error: 9.0242\n",
      "Epoch 16/1000\n",
      "700/700 [==============================] - 0s 89us/step - loss: 532.2429 - mean_absolute_error: 8.9473 - val_loss: 551.0706 - val_mean_absolute_error: 9.0664\n",
      "Epoch 17/1000\n",
      "700/700 [==============================] - 0s 96us/step - loss: 531.7575 - mean_absolute_error: 8.9409 - val_loss: 551.2527 - val_mean_absolute_error: 9.1182\n",
      "Epoch 18/1000\n",
      "700/700 [==============================] - 0s 99us/step - loss: 530.9478 - mean_absolute_error: 8.9600 - val_loss: 551.6661 - val_mean_absolute_error: 9.1469\n",
      "Epoch 19/1000\n",
      "700/700 [==============================] - 0s 91us/step - loss: 530.1982 - mean_absolute_error: 9.0079 - val_loss: 551.8280 - val_mean_absolute_error: 9.1397\n",
      "Epoch 20/1000\n",
      "700/700 [==============================] - 0s 93us/step - loss: 529.5374 - mean_absolute_error: 9.0159 - val_loss: 552.0903 - val_mean_absolute_error: 9.1619\n",
      "Epoch 21/1000\n",
      "700/700 [==============================] - 0s 94us/step - loss: 529.0345 - mean_absolute_error: 8.9977 - val_loss: 552.2529 - val_mean_absolute_error: 9.1835\n",
      "Epoch 22/1000\n",
      "700/700 [==============================] - 0s 96us/step - loss: 528.0510 - mean_absolute_error: 8.9961 - val_loss: 552.6206 - val_mean_absolute_error: 9.2799\n",
      "Epoch 23/1000\n",
      "700/700 [==============================] - 0s 99us/step - loss: 527.6182 - mean_absolute_error: 9.0286 - val_loss: 552.8365 - val_mean_absolute_error: 9.2212\n",
      "Epoch 24/1000\n",
      " 32/700 [>.............................] - ETA: 0s - loss: 294.2373 - mean_absolute_error: 5.5473"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-0591744e06e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_history = history.history['val_mean_absolute_error']\n",
    "all_mae_histories.append(mae_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
